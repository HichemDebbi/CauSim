{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-11T22:21:06.592527Z",
     "iopub.status.busy": "2021-09-11T22:21:06.592213Z",
     "iopub.status.idle": "2021-09-11T22:21:07.005101Z",
     "shell.execute_reply": "2021-09-11T22:21:07.004207Z",
     "shell.execute_reply.started": "2021-09-11T22:21:06.592494Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:21:13.803239Z",
     "iopub.status.busy": "2021-09-11T22:21:13.802606Z",
     "iopub.status.idle": "2021-09-11T22:21:14.054985Z",
     "shell.execute_reply": "2021-09-11T22:21:14.053963Z",
     "shell.execute_reply.started": "2021-09-11T22:21:13.803204Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:57:26.351196Z",
     "iopub.status.busy": "2021-09-11T21:57:26.350913Z",
     "iopub.status.idle": "2021-09-11T21:57:26.374626Z",
     "shell.execute_reply": "2021-09-11T21:57:26.373915Z",
     "shell.execute_reply.started": "2021-09-11T21:57:26.351166Z"
    }
   },
   "outputs": [],
   "source": [
    "traindf = pd.read_csv('../input/datacsv/train.csv', dtype=str)\n",
    "testdf = pd.read_csv('../input/datacsv/test.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:57:28.842678Z",
     "iopub.status.busy": "2021-09-11T21:57:28.841894Z",
     "iopub.status.idle": "2021-09-11T21:57:28.869412Z",
     "shell.execute_reply": "2021-09-11T21:57:28.868702Z",
     "shell.execute_reply.started": "2021-09-11T21:57:28.842629Z"
    }
   },
   "outputs": [],
   "source": [
    "traindf['image_id'] = traindf['image_id'] + \".jpg\"\n",
    "testdf['image_id'] = testdf['image_id'] + \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:57:31.383508Z",
     "iopub.status.busy": "2021-09-11T21:57:31.382941Z",
     "iopub.status.idle": "2021-09-11T21:57:31.515038Z",
     "shell.execute_reply": "2021-09-11T21:57:31.513467Z",
     "shell.execute_reply.started": "2021-09-11T21:57:31.38347Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                             rotation_range=40,\n",
    "                             width_shift_range=0.2, \n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=True,\n",
    "                             fill_mode='nearest')\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "dataframe=traindf,\n",
    "directory=\"\",\n",
    "x_col=\"image_id\",\n",
    "y_col=\"category\",\n",
    "subset=\"training\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:57:34.614679Z",
     "iopub.status.busy": "2021-09-11T21:57:34.614197Z",
     "iopub.status.idle": "2021-09-11T21:57:34.621805Z",
     "shell.execute_reply": "2021-09-11T21:57:34.621138Z",
     "shell.execute_reply.started": "2021-09-11T21:57:34.614643Z"
    }
   },
   "outputs": [],
   "source": [
    "tabLabels=[]\n",
    "for i in range(1, 103):\n",
    "    tabLabels.append(i)\n",
    "labels = tabLabels\n",
    "# labels = dict((v,k) for k,v in labels.items())\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:57:37.557362Z",
     "iopub.status.busy": "2021-09-11T21:57:37.556782Z",
     "iopub.status.idle": "2021-09-11T21:57:40.492041Z",
     "shell.execute_reply": "2021-09-11T21:57:40.491329Z",
     "shell.execute_reply.started": "2021-09-11T21:57:37.557327Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (224, 224, 3))\n",
    "\n",
    "for layer in vgg_model.layers[:-5]:\n",
    "    layer.trainable=False\n",
    "    \n",
    "for layer in vgg_model.layers[1:4]:\n",
    "    layer.trainable=True\n",
    "    \n",
    "input = Input(shape=(224, 224, 3),name = 'image_input')\n",
    "output_vgg16_conv = vgg_model(input)\n",
    "\n",
    "x = BatchNormalization()(output_vgg16_conv)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(102, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input, outputs=x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:57:51.783162Z",
     "iopub.status.busy": "2021-09-11T21:57:51.782569Z",
     "iopub.status.idle": "2021-09-11T21:57:55.577554Z",
     "shell.execute_reply": "2021-09-11T21:57:55.576795Z",
     "shell.execute_reply.started": "2021-09-11T21:57:51.783124Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"../input/flowers-vgg19-weights/flowers_vgg19.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:58:15.489046Z",
     "iopub.status.busy": "2021-09-11T21:58:15.488782Z",
     "iopub.status.idle": "2021-09-11T21:58:15.49823Z",
     "shell.execute_reply": "2021-09-11T21:58:15.497212Z",
     "shell.execute_reply.started": "2021-09-11T21:58:15.489018Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "\n",
    "def output(location, new_model):\n",
    "    img = load_img(location, target_size = (224, 224, 3))\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    answer = new_model.predict(img)\n",
    "    y_class = answer.argmax(axis = -1)\n",
    "    label = labels[np.argmax(answer)]\n",
    "    print('label is', label)\n",
    "    \n",
    "    top_3 = np.argsort(answer[0])[:-4:-1]\n",
    "#     for i in range(3):\n",
    "#         print(\" ({:.3})\".format(answer[0][top_3[i]]))\n",
    "    \n",
    "    y = \" \".join(str(x) for x in y_class)\n",
    "    y = int(y)\n",
    "    res = labels[y]\n",
    "    print(\"category\", res)\n",
    "#     print(\" ({:.3})\".format(answer[0][top_3[0]]))\n",
    "    pred_prob = answer[0][top_3[0]]\n",
    "    print(\" ({:.3})\".format(pred_prob))\n",
    "    \n",
    "#     print (\"new ********************\")   \n",
    "#     proba = answer[0]\n",
    "#     idxs = np.argsort(proba)[::-1][:3]\n",
    "#     for (i, j) in enumerate(idxs):\n",
    "# #         y = labels[i]        \n",
    "#         print(labels[i])\n",
    "#     print(\"new prob\", proba)\n",
    "    return float(pred_prob), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:58:19.205213Z",
     "iopub.status.busy": "2021-09-11T21:58:19.204514Z",
     "iopub.status.idle": "2021-09-11T21:58:25.58489Z",
     "shell.execute_reply": "2021-09-11T21:58:25.584106Z",
     "shell.execute_reply.started": "2021-09-11T21:58:19.205174Z"
    }
   },
   "outputs": [],
   "source": [
    "location = \"../input/classes/flowers_classes_top/1/100.jpg\"\n",
    "output(location, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:58:31.806643Z",
     "iopub.status.busy": "2021-09-11T21:58:31.805998Z",
     "iopub.status.idle": "2021-09-11T21:58:31.821555Z",
     "shell.execute_reply": "2021-09-11T21:58:31.820752Z",
     "shell.execute_reply.started": "2021-09-11T21:58:31.806602Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from pathlib import Path\n",
    "\n",
    "#folder=\"/content/drive/My Drive/Colab/1KDataSet_Part1\"\n",
    "folder = \"../input/classes/flowers_classes_top/\"\n",
    "\n",
    "last_fileName=\"\"\n",
    "#images = []\n",
    "imagesFileNames = []\n",
    "for filename in os.listdir(folder):\n",
    "    # for img in os.listdir(folder+filename):\n",
    "    imagesFileNames.append(filename)\n",
    "    # print(filename)\n",
    "print(len(imagesFileNames))\n",
    "\n",
    "for i in range(0, len(imagesFileNames)) :\n",
    "    imagesFileNames[i] = int(imagesFileNames[i])\n",
    "imagesFileNames.sort()\n",
    "\n",
    "# imagesFileNames = selection_sort(imagesFileNames)\n",
    "# imagesFileNames.sort()\n",
    "for i in range(0, len(imagesFileNames)) :\n",
    "    imagesFileNames[i] = str(imagesFileNames[i])\n",
    "imagesFileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:58:37.24391Z",
     "iopub.status.busy": "2021-09-11T21:58:37.243316Z",
     "iopub.status.idle": "2021-09-11T21:58:37.251263Z",
     "shell.execute_reply": "2021-09-11T21:58:37.250343Z",
     "shell.execute_reply.started": "2021-09-11T21:58:37.243873Z"
    }
   },
   "outputs": [],
   "source": [
    "arryWeights_last_After =[]\n",
    "for i in range(512):\n",
    "  arryWeights_last_After.append(i)\n",
    "for i in range(512):\n",
    "  arryWeights_last_After[i]=1\n",
    "\n",
    "def custom_layer_last(tensor):\n",
    "    return tensor * arryWeights_last_After[i]\n",
    "# vgg_weights = VGG16(    input_shape = IMAGE_SIZE + [3], weights = 'imagenet',\n",
    "#     include_top = False).get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:00:12.418907Z",
     "iopub.status.busy": "2021-09-11T22:00:12.418632Z",
     "iopub.status.idle": "2021-09-11T22:00:12.424876Z",
     "shell.execute_reply": "2021-09-11T22:00:12.423937Z",
     "shell.execute_reply.started": "2021-09-11T22:00:12.418875Z"
    }
   },
   "outputs": [],
   "source": [
    "def insert_intermediate_layer_in_keras(model, layer_id, new_layer):\n",
    "    from tensorflow.keras.models import Model\n",
    "\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    x = layers[0].output\n",
    "    for i in range(1, len(layers)):\n",
    "        if i == layer_id:\n",
    "            x = new_layer(x)\n",
    "        x = layers[i](x)\n",
    "\n",
    "    model = Model(inputs=layers[0].input, outputs=x) ### inputs instead of input in this version\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T21:58:43.930492Z",
     "iopub.status.busy": "2021-09-11T21:58:43.930176Z",
     "iopub.status.idle": "2021-09-11T21:58:43.942458Z",
     "shell.execute_reply": "2021-09-11T21:58:43.941495Z",
     "shell.execute_reply.started": "2021-09-11T21:58:43.930459Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    vgg_model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (224, 224, 3))\n",
    "\n",
    "    for layer in vgg_model.layers[:-5]:\n",
    "        layer.trainable=False\n",
    "\n",
    "    for layer in vgg_model.layers[1:4]:\n",
    "        layer.trainable=True\n",
    "\n",
    "    input = Input(shape=(224, 224, 3),name = 'image_input')\n",
    "    output_vgg16_conv = vgg_model(input)\n",
    "\n",
    "    x = BatchNormalization()(output_vgg16_conv)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(102, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part for computing the CI and storing it into Dictionnary in tx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-11T22:20:46.262238Z",
     "iopub.status.busy": "2021-09-11T22:20:46.2619Z",
     "iopub.status.idle": "2021-09-11T22:20:52.247612Z",
     "shell.execute_reply": "2021-09-11T22:20:52.24559Z",
     "shell.execute_reply.started": "2021-09-11T22:20:46.262154Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, MaxPool2D \n",
    "\n",
    "# from kerassurgeon import Surgeon\n",
    "# from kerassurgeon import identify\n",
    "# from kerassurgeon import utils\n",
    "# from kerassurgeon.operations import delete_channels\n",
    "# dctOfAllDictionClasses = {}\n",
    "\n",
    "weights_dic = {}\n",
    "\n",
    "imgCount = 0\n",
    "indexImg=0\n",
    "# img_path = 'Mountain_Bike/[image-net.org][27]443691662_09f12b8e37.jpg'\n",
    "for imgFileName in imagesFileNames:\n",
    "    dctOfAllDictionClasses = {}\n",
    "    imgPath = folder + imagesFileNames[indexImg] + \"/\" + os.listdir(folder + imagesFileNames[indexImg])[0]\n",
    "    print(imgPath)\n",
    "    res = output(imgPath, model)\n",
    "    print('res[0]', res[0])\n",
    "    print('res[1]', res[1])\n",
    "    originalName = imgFileName\n",
    "    # originalName = res[1]\n",
    "    # if '.jpg' in originalName:\n",
    "    #     originalName = originalName.replace('.jpg','')\n",
    "    print('orign name:', originalName)\n",
    "    class_Id = res[1]\n",
    "    orig_acc = res[0]\n",
    "    print('origin acc', orig_acc)\n",
    "    weights_dic = {}\n",
    "    K.set_learning_phase(0)\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "       \n",
    "    for i in range(512):\n",
    "        print ('filt{}'.format (i))\n",
    "        arryWeights_last_After[i]=0\n",
    "        # def custom_layer_last(tensor):\n",
    "        #   return tensor * arryWeights_last_After[i]\n",
    "        filt = i\n",
    "#         #print(model.summary())\n",
    "#         #model = insert_intermediate_layer_in_keras(model, 4, BatchNormalization()\n",
    "        def custom_layer_last(tensor):\n",
    "            # print ('tesnor values :', tensor[0][0])\n",
    "            return tensor * arryWeights_last_After # working version\n",
    "\n",
    "#         new_model = build_model()\n",
    "        lambda_layer = Lambda (custom_layer_last, name=\"lambda_New\")\n",
    "        new_model = build_model()\n",
    "        new_model.load_weights('../input/flowers-vgg19-weights/flowers_vgg19.h5')\n",
    "\n",
    "        new_model.layers[1] = insert_intermediate_layer_in_keras (new_model.layers[1], 21, lambda_layer)\n",
    "\n",
    "        img = load_img(imgPath, target_size = (224, 224, 3))\n",
    "        img = img_to_array(img)\n",
    "        img = img / 255\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        answer = new_model.predict(img)\n",
    "        y_class = answer.argmax(axis = -1)\n",
    "\n",
    "        top_3 = np.argsort(answer[0])[:-4:-1]\n",
    "        New_accu = answer[0][top_3[0]]\n",
    "        print(\" ({:.3})\".format(New_accu))\n",
    "        print('label name:', res)\n",
    "        print ('acc after purning (%.2f%%)', New_accu)\n",
    "        print('difference: ', (orig_acc - New_accu))\n",
    "        #weights_dic[filt][0] = 1/(k+1)  # resp\n",
    "        tuple = orig_acc - New_accu, 1\n",
    "        #weights_dic[filt] = original_loss[1] - loss[1]  # prob\n",
    "        weights_dic[filt] = tuple\n",
    "        arryWeights_last_After[i]=1\n",
    "\n",
    "    weights_dic_sort = sorted (weights_dic.items (), key=lambda kv: kv[1], reverse=True)\n",
    "    #     print ('Resp and loss for conv layer {}\\n'.format (1), weights_dic_sort)\n",
    "    # arryOfDictFiltByLayer.append(weights_dic_sort)\n",
    "    #K.backend.clear_session ()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    pairKeyValue = { originalName: weights_dic_sort}\n",
    "    dctOfAllDictionClasses = {}\n",
    "    dctOfAllDictionClasses.update(pairKeyValue)\n",
    "    print('pair ley:', pairKeyValue)\n",
    "    import json\n",
    "    #     with open('FIltersRespAll', 'w') as fout:\n",
    "    #         json.dump(str(lstOfAllDictionClasses), fout)\n",
    "    if os.path.exists('../output/kaggle/working/Filters_Resp_Flowers.txt'):\n",
    "        append_write = 'a' # append if already exists\n",
    "    else:\n",
    "        append_write = 'w+' # make a new file if not\n",
    "    with open('../output/kaggle/working/Filters_Resp_Flowers.txt', append_write) as fout:\n",
    "        fout.write(\"\\n\" + str(dctOfAllDictionClasses) + \"\\n\") \n",
    "        print('Last Image ID:', imagesFileNames[imgCount])\n",
    "    imgCount+=1\n",
    "    indexImg+=1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
