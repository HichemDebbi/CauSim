{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27488,
     "status": "ok",
     "timestamp": 1640903060267,
     "user": {
      "displayName": "Hichem Debbi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghlb6k8JnEN2iGpLY203eW79uSvr3gClJM-KxBu=s64",
      "userId": "12786207588668129567"
     },
     "user_tz": -60
    },
    "id": "HHGLF40jWO3r",
    "outputId": "e4c91dd4-4d4d-473f-d51e-3703cf3a59da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1640903066389,
     "user": {
      "displayName": "Hichem Debbi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghlb6k8JnEN2iGpLY203eW79uSvr3gClJM-KxBu=s64",
      "userId": "12786207588668129567"
     },
     "user_tz": -60
    },
    "id": "Ige55TLQWRzW"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load_and_preprocess_image(path):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (224,224))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1640903096692,
     "user": {
      "displayName": "Hichem Debbi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghlb6k8JnEN2iGpLY203eW79uSvr3gClJM-KxBu=s64",
      "userId": "12786207588668129567"
     },
     "user_tz": -60
    },
    "id": "SyCUA1_EWO30"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from keras.layers import Lambda\n",
    "import tensorflow\n",
    "def outer_product(x):\n",
    "    \"\"\"\n",
    "    calculate outer-products of 2 tensors\n",
    "\n",
    "        args \n",
    "            x\n",
    "                list of 2 tensors\n",
    "                , assuming each of which has shape = (size_minibatch, total_pixels, size_filter)\n",
    "    \"\"\"\n",
    "    return tensorflow.keras.backend.batch_dot(\n",
    "                x[0]\n",
    "                , x[1]\n",
    "                , axes=[1,1]\n",
    "            ) / x[0].get_shape().as_list()[1] \n",
    "\n",
    "def signed_sqrt(x):\n",
    "    \"\"\"\n",
    "    calculate element-wise signed square root\n",
    "\n",
    "        args\n",
    "            x\n",
    "                a tensor\n",
    "    \"\"\"\n",
    "    return tensorflow.keras.backend.sign(x) * tensorflow.keras.backend.sqrt(tensorflow.keras.backend.abs(x) + 1e-9)\n",
    "\n",
    "def L2_norm(x, axis=-1):\n",
    "    \"\"\"\n",
    "    calculate L2-norm\n",
    "\n",
    "        args \n",
    "            x\n",
    "                a tensor\n",
    "    \"\"\"\n",
    "    return tensorflow.keras.backend.l2_normalize(x, axis=axis)\n",
    "\n",
    "\n",
    "def build_model(filter_zero##added aprameter\n",
    "                ,value_filter\n",
    "    ,size_heigth=224\n",
    "    ,size_width=224\n",
    "    ,no_class=200\n",
    "    ,no_last_layer_backbone=17\n",
    "    \n",
    "    ,name_optimizer=\"sgd\"\n",
    "    ,rate_learning=1.0\n",
    "    ,rate_decay_learning=0.0\n",
    "    ,rate_decay_weight=0.0\n",
    "    \n",
    "    ,name_initializer=\"glorot_normal\"\n",
    "    ,name_activation_logits=\"softmax\"\n",
    "    ,name_loss=\"categorical_crossentropy\"\n",
    "    ,flg_debug=False\n",
    "    ,**kwargs\n",
    "):\n",
    "    \n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    \n",
    "    # print(\"-------------------------------\")\n",
    "    # print(\"parameters:\")\n",
    "    # for key, val in locals().items():\n",
    "    #     if not val == None and not key == \"kwargs\":\n",
    "    #         print(\"\\t\", key, \"=\",  val)\n",
    "    # print(\"-------------------------------\")\n",
    "    \n",
    "    ### \n",
    "    ### load pre-trained model\n",
    "    ###\n",
    "    tensor_input = tensorflow.keras.layers.Input(shape=[size_heigth,size_width,3])\n",
    "    model_detector = tensorflow.keras.applications.vgg16.VGG16(\n",
    "                            input_tensor=tensor_input\n",
    "                            , include_top=False\n",
    "                            , weights='imagenet'\n",
    "                        )\n",
    "    \n",
    "    arryWeights_last_After =[]\n",
    "    for i in range(512):\n",
    "      arryWeights_last_After.append(i)\n",
    "    for i in range(512):\n",
    "      arryWeights_last_After[i]=1\n",
    "    \n",
    "    arryWeights_last_After[filter_zero]=value_filter\n",
    "    # print('filter' , filter_zero, \"=\", arryWeights_last_After[filter_zero])\n",
    "    def custom_layer_last(tensor):\n",
    "        return tensor * arryWeights_last_After\n",
    "    lambda_layer = Lambda (custom_layer_last, name=\"lambda_New\")\n",
    "\n",
    "    model_detector = insert_intermediate_layer_in_keras (model_detector, 18, lambda_layer)\n",
    "    # print('model detector')\n",
    "    model_detector.summary()\n",
    "    ### \n",
    "    ### bi-linear pooling\n",
    "    ###\n",
    "\n",
    "    # extract features from detector\n",
    "    x_detector = model_detector.layers[no_last_layer_backbone].output\n",
    "    shape_detector = model_detector.layers[no_last_layer_backbone].output_shape\n",
    "    # if flg_debug:\n",
    "    #     print(\"shape_detector : {}\".format(shape_detector))\n",
    "\n",
    "    # extract features from extractor , same with detector for symmetry DxD model\n",
    "    shape_extractor = shape_detector\n",
    "    x_extractor = x_detector\n",
    "    # if flg_debug:\n",
    "    #     print(\"shape_extractor : {}\".format(shape_extractor))\n",
    "        \n",
    "    \n",
    "    # rehape to (minibatch_size, total_pixels, filter_size)\n",
    "    x_detector = tensorflow.keras.layers.Reshape(\n",
    "            [\n",
    "                shape_detector[1] * shape_detector[2] , shape_detector[-1]\n",
    "            ]\n",
    "        )(x_detector)\n",
    "    # if flg_debug:\n",
    "    #     print(\"x_detector shape after rehsape ops : {}\".format(x_detector.shape))\n",
    "        \n",
    "    x_extractor = tensorflow.keras.layers.Reshape(\n",
    "            [\n",
    "                shape_extractor[1] * shape_extractor[2] , shape_extractor[-1]\n",
    "            ]\n",
    "        )(x_extractor)\n",
    "    # if flg_debug:\n",
    "    #     print(\"x_extractor shape after rehsape ops : {}\".format(x_extractor.shape))\n",
    "        \n",
    "        \n",
    "    # outer products of features, output shape=(minibatch_size, filter_size_detector*filter_size_extractor)\n",
    "    x = tensorflow.keras.layers.Lambda(outer_product)(\n",
    "        [x_detector, x_extractor]\n",
    "    )\n",
    "    # if flg_debug:\n",
    "    #     print(\"x shape after outer products ops : {}\".format(x.shape))\n",
    "        \n",
    "        \n",
    "    # rehape to (minibatch_size, filter_size_detector*filter_size_extractor)\n",
    "    x = tensorflow.keras.layers.Reshape([shape_detector[-1]*shape_extractor[-1]])(x)\n",
    "    # if flg_debug:\n",
    "    #     print(\"x shape after rehsape ops : {}\".format(x.shape))\n",
    "        \n",
    "        \n",
    "    # signed square-root \n",
    "    x = tensorflow.keras.layers.Lambda(signed_sqrt)(x)\n",
    "    # if flg_debug:\n",
    "    #     print(\"x shape after signed-square-root ops : {}\".format(x.shape))\n",
    "        \n",
    "    # L2 normalization\n",
    "    x = tensorflow.keras.layers.Lambda(L2_norm)(x)\n",
    "    # if flg_debug:\n",
    "    #     print(\"x shape after L2-Normalization ops : {}\".format(x.shape))\n",
    "\n",
    "\n",
    "\n",
    "    ### \n",
    "    ### attach FC-Layer\n",
    "    ###\n",
    "\n",
    "    if name_initializer != None:\n",
    "            name_initializer = eval(name_initializer+\"()\")\n",
    "            \n",
    "    x = tensorflow.keras.layers.Dense(\n",
    "            units=no_class\n",
    "            ,kernel_regularizer=tensorflow.keras.regularizers.l2(rate_decay_weight)\n",
    "            ,kernel_initializer=name_initializer\n",
    "        )(x)\n",
    "    # if flg_debug:\n",
    "    #     print(\"x shape after Dense ops : {}\".format(x.shape))\n",
    "    tensor_prediction = tensorflow.keras.layers.Activation(name_activation_logits)(x)\n",
    "    # if flg_debug:\n",
    "    #     print(\"prediction shape : {}\".format(tensor_prediction.shape))\n",
    "\n",
    "        \n",
    "\n",
    "    ### \n",
    "    ### compile model\n",
    "    ###\n",
    "    model_bilinear = tensorflow.keras.models.Model(\n",
    "                        inputs=[tensor_input]\n",
    "                        , outputs=[tensor_prediction]\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    # fix pre-trained weights\n",
    "    for layer in model_detector.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        \n",
    "    # define optimizers\n",
    "    opt_adam = tensorflow.keras.optimizers.Adam(\n",
    "                    lr=rate_learning\n",
    "                    , decay=rate_decay_learning\n",
    "                )\n",
    "    opt_rms = tensorflow.keras.optimizers.RMSprop(\n",
    "                    lr=rate_learning\n",
    "                    , decay=rate_decay_learning\n",
    "                )\n",
    "    opt_sgd = tensorflow.keras.optimizers.SGD(\n",
    "                    lr=rate_learning\n",
    "                    , decay=rate_decay_learning\n",
    "                    , momentum=0.9\n",
    "                    , nesterov=False\n",
    "                )\n",
    "    optimizers ={\n",
    "        \"adam\":opt_adam\n",
    "        ,\"rmsprop\":opt_rms\n",
    "        ,\"sgd\":opt_sgd\n",
    "    }\n",
    "    \n",
    "    model_bilinear.compile(\n",
    "        loss=name_loss\n",
    "        , optimizer=optimizers[name_optimizer]\n",
    "        , metrics=[\"categorical_accuracy\"]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if flg_debug:\n",
    "    #     model_bilinear.summary()\n",
    "    \n",
    "    return model_bilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1640903102603,
     "user": {
      "displayName": "Hichem Debbi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghlb6k8JnEN2iGpLY203eW79uSvr3gClJM-KxBu=s64",
      "userId": "12786207588668129567"
     },
     "user_tz": -60
    },
    "id": "V1pynNdNEmXV"
   },
   "outputs": [],
   "source": [
    "def insert_intermediate_layer_in_keras(model, layer_id, new_layer):\n",
    "    from keras.models import Model\n",
    "\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    x = layers[0].output\n",
    "    for i in range(1, len(layers)):\n",
    "        if i == layer_id:\n",
    "            x = new_layer(x)\n",
    "        x = layers[i](x)\n",
    "\n",
    "    model = Model(inputs=layers[0].input, outputs=x) ### inputs instead of input in this version\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3987,
     "status": "ok",
     "timestamp": 1640903109353,
     "user": {
      "displayName": "Hichem Debbi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghlb6k8JnEN2iGpLY203eW79uSvr3gClJM-KxBu=s64",
      "userId": "12786207588668129567"
     },
     "user_tz": -60
    },
    "id": "AVdsbnsECWDc",
    "outputId": "7b459fe0-995b-4299-e8ea-8c241f5e7e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "58900480/58889256 [==============================] - 1s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " lambda_New (Lambda)         (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(filter_zero=0\n",
    "                    , value_filter=1\n",
    "            # number of output classes, 200 for CUB200\n",
    "            ,no_class = 120\n",
    "\n",
    "            # pretrained model specification, using VGG16\n",
    "            # \"block5_conv3 \"\n",
    "            ,no_last_layer_backbone = 18\n",
    "\n",
    "            # training parametes\n",
    "            ,rate_learning=1.0\n",
    "            ,rate_decay_weight=1e-8\n",
    "            ,flg_debug=True\n",
    "        )\n",
    "\n",
    "# model = build_model(\n",
    "#             # number of output classes, 200 for CUB200\n",
    "#             no_class = NO_CLASS\n",
    "\n",
    "#             # pretrained model specification, using VGG16\n",
    "#             # \"block5_conv3 \"\n",
    "#             ,no_last_layer_backbone = 17           \n",
    "#             # training parametes\n",
    "#             ,rate_learning=1.0\n",
    "#             ,rate_decay_weight=1e-8\n",
    "#             ,flg_debug=True\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzWZmMIqEoYG"
   },
   "outputs": [],
   "source": [
    "# now all layers are trainable\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# change LR\n",
    "opt_sgd = tensorflow.keras.optimizers.SGD(\n",
    "                lr=1e-3\n",
    "                , decay=1e-9\n",
    "                , momentum=0.9\n",
    "                , nesterov=False\n",
    "            )\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\"\n",
    "    , optimizer=opt_sgd\n",
    "    , metrics=[\"categorical_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20DD5I7iWO30"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"/content/drive/My Drive/Colab Notebooks/Data_Stanford_Dogs/BCNN_dogs_weights_new_last_Best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnsVVwloVynN"
   },
   "outputs": [],
   "source": [
    "layer = model.layers[17]\n",
    "layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-CbwDpSWO31"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dogs_labels = set()\n",
    "\n",
    "path= \"/content/drive/My Drive/Colab Notebooks/Data_Stanford_Dogs/images/Images\"\n",
    "for d in os.listdir(path):\n",
    "    dogs_labels.add(d)\n",
    "\n",
    "len(dogs_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gU5zTQe1WO32"
   },
   "outputs": [],
   "source": [
    "dogs_labels = list(dogs_labels)\n",
    "# dogs_labels.sort()\n",
    "dogs_labels\n",
    "\n",
    "# dogs_labels = list(dogs_labels)\n",
    "# dogs_labels_path = [path + s for s in dogs_labels]\n",
    "# # dogs_labels\n",
    "dogs_labels = list(dogs_labels)\n",
    "dogs_labels.sort()\n",
    "dogs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xvneJl5PnHE"
   },
   "outputs": [],
   "source": [
    "# dogs_labels = list(dogs_labels)\n",
    "# dogs_labels.sort()\n",
    "# dogs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOQPJYwLWO32"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "\n",
    "def output(location, new_model):\n",
    "    img = load_and_preprocess_image(location)\n",
    "#     img = load_img(location, target_size = (224, 224, 3))\n",
    "#     img = img_to_array(img)\n",
    "#     img = img / 255\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    answer = new_model.predict(img)\n",
    "    y_class = answer.argmax(axis = -1)\n",
    "    \n",
    "    top_3 = np.argsort(answer[0])[:-4:-1]\n",
    "    # for i in range(3):\n",
    "    #     print(\" ({:.3})\".format(answer[0][top_3[i]]))\n",
    "    \n",
    "    y = \" \".join(str(x) for x in y_class)\n",
    "    y = int(y)\n",
    "    res = dogs_labels[y]\n",
    "    # print(res)\n",
    "#     print(\" ({:.3})\".format(answer[0][top_3[0]]))\n",
    "    pred_prob = answer[0][top_3[0]]\n",
    "    # print(\" ({:.3})\".format(pred_prob))\n",
    "    \n",
    "#     print (\"new ********************\")   \n",
    "#     proba = answer[0]\n",
    "#     idxs = np.argsort(proba)[::-1][:3]\n",
    "#     for (i, j) in enumerate(idxs):\n",
    "          #y = dogs_labels[i]        \n",
    "#         print(y)\n",
    "#     print(\"new prob\", proba)\n",
    "#     print(\"***************** new *******\")\n",
    "#     pred = new_model.predict(img)[0]\n",
    "#     ans = pred.argsort()[-3:][::-1]\n",
    "#     print(\"That's most likely a\", dogs_labels[ans[0]]) \n",
    "#     print(\"But it might also be a\", dogs_labels[ans[1]], \"or a\", dogs_labels[ans[2]])\n",
    "    return float(pred_prob), res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u-zDDcO9gpa"
   },
   "outputs": [],
   "source": [
    "location = \"/content/drive/My Drive/Colab Notebooks/Data_Stanford_Dogs/dogs_classes_top/n02106166-Border_collie/n02106166_18.jpg\"\n",
    "output(location, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying top prototypes dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Gg2NVDr0w11"
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# from shutil import copyfile\n",
    "# from pathlib import Path\n",
    "\n",
    "# folder = \"/content/drive/My Drive/Colab Notebooks/Data_Stanford_Dogs/images/Images/\"\n",
    "# folder_Copy = \"/content/drive/My Drive/Colab Notebooks/Data_Stanford_Dogs/dogs_classes_top/\"\n",
    "\n",
    "# last_fileName=\"\"\n",
    "\n",
    "# #images = []\n",
    "# imagesFileNames = []\n",
    "# for filename in os.listdir(folder):\n",
    "#     # if filename not in os.listdir(folder_Copy):\n",
    "#     for img in os.listdir(folder+filename):\n",
    "#       if output(folder+filename+\"/\"+img, model)[0] >= 0.1 :\n",
    "#           print(folder+filename+\"/\"+img, model)\n",
    "#           if output(folder+filename+\"/\"+img, model)[1] ==  filename:\n",
    "#               print('path', folder+filename+\"/\"+img)\n",
    "#               print('prob=', output(folder+filename+\"/\"+img, model)[0])\n",
    "#               src = folder+filename+\"/\"+img\n",
    "#               dst = folder_Copy+filename+\"/\"+img\n",
    "#               # Path(folder_Copy+filename+\"/\").mkdir(parents=True, exist_ok=True)\n",
    "#               # copyfile(src, dst)\n",
    "#               print(filename)\n",
    "#   #             print(img)\n",
    "#               break;\n",
    "#       #     img = cv2.imread(os.path.join(folder,filename))\n",
    "#       #     if img is not None:\n",
    "#       #         #images.append(img)\n",
    "#           imagesFileNames.append(filename)\n",
    "# imagesFileNames.sort()        \n",
    "# # print(imagesFileNames[0])\n",
    "# #images = load_images_from_folder(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7p9ymX7dWO39"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "folder = \"/content/drive/My Drive/Colab Notebooks/Data_Stanford_Dogs/dogs_classes_top/\"\n",
    "\n",
    "last_fileName=\"\"\n",
    "#images = []\n",
    "imagesFileNames = []\n",
    "for filename in os.listdir(folder):\n",
    "    # for img in os.listdir(folder+filename):\n",
    "    imagesFileNames.append(filename)\n",
    "    print(filename)\n",
    "imagesFileNames.sort()\n",
    "print(len(imagesFileNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycZ_qiFNWO39"
   },
   "outputs": [],
   "source": [
    "arryWeights_last_After =[]\n",
    "for i in range(512):\n",
    "  arryWeights_last_After.append(i)\n",
    "for i in range(512):\n",
    "  arryWeights_last_After[i]=1\n",
    "\n",
    "def custom_layer_last(tensor):\n",
    "    return tensor * arryWeights_last_After[i]\n",
    "# vgg_weights = VGG16(    input_shape = IMAGE_SIZE + [3], weights = 'imagenet',\n",
    "#     include_top = False).get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IYnujAxWO3-"
   },
   "outputs": [],
   "source": [
    "def insert_intermediate_layer_in_keras(model, layer_id, new_layer):\n",
    "    from keras.models import Model\n",
    "\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    x = layers[0].output\n",
    "    for i in range(1, len(layers)):\n",
    "        if i == layer_id:\n",
    "            x = new_layer(x)\n",
    "        x = layers[i](x)\n",
    "\n",
    "    model = Model(inputs=layers[0].input, outputs=x) ### inputs instead of input in this version\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part for computing the CI and storing it into Dictionnary in tx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7jMAS81WO3-"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, MaxPool2D \n",
    "\n",
    "# from kerassurgeon import Surgeon\n",
    "# from kerassurgeon import identify\n",
    "# from kerassurgeon import utils\n",
    "# from kerassurgeon.operations import delete_channels\n",
    "# dctOfAllDictionClasses = {}\n",
    "\n",
    "weights_dic = {}\n",
    "NO_CLASS = 120\n",
    "imgCount = 0\n",
    "indexImg=0\n",
    "# img_path = 'Mountain_Bike/[image-net.org][27]443691662_09f12b8e37.jpg'\n",
    "for imgFileName in imagesFileNames:\n",
    "    dctOfAllDictionClasses = {}\n",
    "    imgPath = folder + imagesFileNames[indexImg] + \"/\" + os.listdir(folder + imagesFileNames[indexImg])[0]\n",
    "    print(imgPath)\n",
    "    res = output(imgPath, model)\n",
    "    # print('res[0]', res[0])\n",
    "    # print('res[1]', res[1])\n",
    "    originalName = imgFileName\n",
    "    # originalName = res[1]\n",
    "    # if '.jpg' in originalName:\n",
    "    #     originalName = originalName.replace('.jpg','')\n",
    "    print('orign name:', originalName)\n",
    "    class_Id = res[1]\n",
    "    orig_acc = res[0]\n",
    "    print('origin acc', orig_acc)\n",
    "    from keras.layers import Lambda\n",
    "    weights_dic = {}\n",
    "    K.set_learning_phase(0)\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    for i in range(512):\n",
    "        print ('filt{}'.format (i))\n",
    "        # arryWeights_last_After[i]=0\n",
    "        # def custom_layer_last(tensor):\n",
    "        #   return tensor * arryWeights_last_After[i]\n",
    "        filt = i\n",
    "\n",
    "        new_model = build_model( filter_zero=i##added aprameter\n",
    "            ,value_filter =0\n",
    "            # number of output classes, 200 for CUB200\n",
    "            ,no_class = NO_CLASS\n",
    "\n",
    "            # pretrained model specification, using VGG16\n",
    "            # \"block5_conv3 \"\n",
    "            ,no_last_layer_backbone = 18\n",
    "                        # training parametes\n",
    "            ,rate_learning=1.0\n",
    "            ,rate_decay_weight=1e-8\n",
    "            ,flg_debug=True\n",
    "        )\n",
    "\n",
    "                # now all layers are trainable\n",
    "        for layer in new_model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # change LR\n",
    "        opt_sgd = tensorflow.keras.optimizers.SGD(\n",
    "                        lr=1e-3\n",
    "                        , decay=1e-9\n",
    "                        , momentum=0.9\n",
    "                        , nesterov=False\n",
    "                    )\n",
    "        new_model.compile(\n",
    "            loss=\"categorical_crossentropy\"\n",
    "            , optimizer=opt_sgd\n",
    "            , metrics=[\"categorical_accuracy\"]\n",
    "        )\n",
    "        new_model.load_weights('/content/drive/My Drive/Colab Notebooks/Data_Stanford_Dogs/BCNN_dogs_weights_new_last_Best.h5')\n",
    "\n",
    "        new_res = output(imgPath, new_model)\n",
    "        # print('res[0]', new_res[0])\n",
    "        # print('res[1]', new_res[1])\n",
    "    \n",
    "        New_accu = new_res[0]\n",
    "        print(\" ({:.3})\".format(New_accu))\n",
    "        print('label name:', new_res[1])\n",
    "        print ('acc after purning (%.2f%%)', New_accu)\n",
    "        print('difference: ', (orig_acc - New_accu))\n",
    "        #weights_dic[filt][0] = 1/(k+1)  # resp\n",
    "        tuple = orig_acc - New_accu, 1\n",
    "        #weights_dic[filt] = original_loss[1] - loss[1]  # prob\n",
    "        weights_dic[filt] = tuple\n",
    "        # arryWeights_last_After[i]=1\n",
    "        K.clear_session ()\n",
    "\n",
    "    weights_dic_sort = sorted (weights_dic.items (), key=lambda kv: kv[1], reverse=True)\n",
    "    #     print ('Resp and loss for conv layer {}\\n'.format (1), weights_dic_sort)\n",
    "    # arryOfDictFiltByLayer.append(weights_dic_sort)\n",
    "    #K.backend.clear_session ()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    pairKeyValue = { originalName: weights_dic_sort}\n",
    "    dctOfAllDictionClasses = {}\n",
    "    dctOfAllDictionClasses.update(pairKeyValue)\n",
    "    print('pair ley:', pairKeyValue)\n",
    "    import json\n",
    "    #     with open('FIltersRespAll', 'w') as fout:\n",
    "    #         json.dump(str(lstOfAllDictionClasses), fout)\n",
    "    # with open('/content/drive/My Drive/Colab Notebooks/Cub-200_BCNN/Filt_Resp_Birds_BCNN.txt', 'a') as fout:\n",
    "    with open('/content/drive/My Drive/Colab Notebooks/Data_Stanford_Dogs/Filt_Resp_Dogs.txt', 'a') as fout:\n",
    "\n",
    "        fout.write(\"\\n\" + str(dctOfAllDictionClasses) + \"\\n\") \n",
    "        print('Last Image ID:', imagesFileNames[imgCount])\n",
    "    imgCount+=1\n",
    "    indexImg+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FWVTd5bSLBq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BbbKPGeWO4C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RespDogsVGG_BCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
